[[ux_persistence]]

The UNICORE persistence layer 
-----------------------------

UNICORE stores its state in data bases. The information that is stored 
depends on the services that are running in the container, and can
include

 - user's resources (instances of storage, job and other services)
 - jobs
 - workflows

etc.

The job directories themselves reside on the target system, but UNICORE keeps 
additional information (like, which UNICORE user owns a particular job).

The data on user resources is organised by service name, i.e. each service 
(for example, JobManagement) stores its information
in a separate database table (having the same name as the service, e.g. "JobManagement").

The UNICORE persistence layer offers three kinds of storage:

 - on the filesystem of the UNICORE/X server (using the H2 database engine), which is generally the default;

 - on a database server (MySQL, or the "server mode" of H2);

 - in-memory, i.e. all info is lost on server restart.

While the first one is very easy to setup, and easy to manage, the second option
allows advanced setups like clustering/load balancing configurations involving multiple
UNICORE/X servers sharing the same persistent data. Using MySQL has the additional 
benefit that the server starts up faster.

Data migration from one database system to another is in principle possible, but you
should select the storage carefully before going into production. In general, if you
do not require clustering/load balancing, you should choose the default filesystem option, 
since it is less administrative effort. 


Configuring the persistence layer
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Peristence properties are configured in two files:

 - container.properties for all service data
 - xnjs.properties for job data

It is recommended to specify a configuration file using the +persistence.config+ property. Thus, 
persistence configuration can be easily shared between the job (XNJS) data and other service data.
If the "persistence.config" property is set, the given file will be read as a Java properties 
file, and the properties will be used.

[NOTE]
===================
All properties can be specified on a "per table" basis, by appending ".<TABLENAME>"
to the property name. This means you can even select different storage systems for
different data, e.g. store service data on the filesystem and jobs in MySQL.
The table name is case-sensitive.
===================

include::persistence-properties.txt[]

==== Caching
  
By default, caching of data in memory is enabled. It can be switched off and configured on a 
per-table (i.e. per entity class) basis. If you have a lot of memory for your server, you might 
consider increasing the cache size for certain components.

For example, to set the maximum size of the JOBS cache to 1000, you'd configure

-------------
persistence.cache.maxSize.JOBS=1000
-------------

==== The H2 engine

H2 is a pure Java database engine. It can be used in embedded mode (i.e. the engine runs in-process), or
in server mode, if multiple UNICORE servers should use the same database server. For more information, visit
http://www.h2database.com


===== Connection URL

In H2 server mode, the connection URL is constructed as follows
  
------
  jdbc:h2:tcp://<persistence.host>:<persistence.port>/<persistence.directory>/<table_name>
------


==== The MySQL Engine

The MySQL database engine does not need an introduction. To configure its use for UNICORE persistence 
data, you need to set

--------------
persistence.class=de.fzj.unicore.persist.impl.MySQLPersist
--------------

To use MySQL, you need access to an installed MySQL server. It is beyond the scope
of this guide to describe in detail how to setup and operate MySQL. 
The following is a simple sequence of steps to be performed for setting up the required 
database structures.

 - open the mysql console

 - create a dedicated user, say 'unicore' who will connect from some server in the domain "yourdomain.com" or
from the local host:
--------------
CREATE USER 'unicore'@'%.yourdomain.com' identified by 'some_password' ;
CREATE USER 'unicore'@'localhost' identified by 'some_password' ;
--------------

 - create a dedicated database for use by the UNICORE/X server:
--------------
CREATE DATABASE 'unicore_data_demo_site';
USE 'unicore_data_demo_site';
--------------

 - allow the unicore user access to that database:

----------------
GRANT ALL PRIVILEGES ON 'unicore_data_demo_site.*' to 'unicore'@'localhost';
GRANT ALL PRIVILEGES ON 'unicore_data_demo_site.*' to 'unicore'@'%.yourdomain.com';
----------------

The UNICORE persistence properties would in this case look like this:

-----------------------
persistence.class=de.fzj.unicore.persist.impl.MySQLPersist 
persistence.database=unicore_data_demo_site
persistence.user=unicore
persistence.password=some_password
persistence.host=<your_mysql_host>
persistence.port=<your_mysql_port>
persistence.mysql.tabletype=MyISAM
-----------------------

If you want to store data from multiple UNICORE servers, make sure to
use a different database for each of them.

Clustering
~~~~~~~~~~

If you intend to run a configuration with multiple UNICORE servers accessing a shared database, you need
to enable clustering mode by setting a property

-------------
persistence.cluster.enable=true
-------------

The clustering config file can be set using a (per-table) property 

-------------
persistence.cluster.config=<path to config file>
-------------
  
If this is not set, a default configuration is used.

For clustering, the Hazelcast library is used (https://hazelcast.org/documentation). 
A basic TCP based configuration is

--------------------
  <hazelcast xmlns="http://www.hazelcast.com/schema/config">
    <group>
        <name>persistence-dev</name>
        <password>dev-pass</password>
    </group>
    <network>
        <port auto-increment="true">5701</port>
        <join>
            <multicast enabled="false"/>
            <tcp-ip enabled="true">
	        <!-- list other members of the cluster -->
                <member>127.0.0.1</member>
		<member>some.host.org</member>
            </tcp-ip>
        </join>
    </network>
  </hazelcast>
--------------------


The most important part is the "tcp-ip" setting, which must list at
least one other node in the cluster.  The "group" setting allows to
run multiple clusters on the same set of hosts, just make sure that
the group name is the same for all nodes in a cluster.

